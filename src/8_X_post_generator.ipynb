{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "903a3a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph,START,END\n",
    "from typing import TypedDict, Literal,Annotated\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage,HumanMessage\n",
    "from pydantic import BaseModel,Field\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16093b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_llm=ChatOpenAI(model=\"gpt-3.5-turbo-0125\",temperature=0.7)\n",
    "evaluator_llm=ChatOpenAI(model=\"gpt-3.5-turbo-0125\",temperature=0.7)\n",
    "optimizer_llm=ChatOpenAI(model=\"gpt-3.5-turbo-0125\",temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bbce6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "\n",
    "class TweetState(TypedDict):\n",
    "    topic:str\n",
    "    tweet:str\n",
    "    evaluation:Literal[\"approved\",\"needs_imporovement\"]\n",
    "    iteration=int\n",
    "    max_iteration:int\n",
    "    feedback: str \n",
    "\n",
    "    tweet_history: Annotated[list[str],operator.add]\n",
    "    feedback_history: Annotated[list[str],operator.add]\n",
    "\n",
    "class TweetEvaluation(BaseModel):\n",
    "    evaluation:Literal[\"approved\",\"needs_improvement\"]=Field(..., description=\"Final evaluation of tweet\")\n",
    "    feedback:str = Field(...,description =\"COnstructive feedback for tweet.\")\n",
    "    # score:int=Field(..., ge=0,le=5,description=\"Total score from rubric (o to 5).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1965a60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/suresh/Desktop/Learning/Lang-Graph-learn/.venv/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:1928: UserWarning: Cannot use method='json_schema' with model gpt-3.5-turbo-0125 since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "structured_evaluator_llm=evaluator_llm.with_structured_output(TweetEvaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e497a33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tweet(state:TweetState):\n",
    "    # prompt\n",
    "    messages=[\n",
    "        SystemMessage(content=\"You are a funny and clever Twitter/X influencer.\"),\n",
    "        HumanMessage(content=f\"\"\"additional_kwargs=\n",
    "                     Write a short, original and hilarious tweet on topic: \"{state['topic']}\".\n",
    "                     Rules:\n",
    "                     - Do not use question-answer format.\n",
    "                     -max 200 character.\n",
    "                     - Use observational humor, irony, sarcasm, or cultural references.\n",
    "                     -think in meme logic, punchlines , or relatable takes.\n",
    "                     - use simple, day to day english.\n",
    "                     \"\"\")\n",
    "    ]\n",
    "    response=generator_llm.invoke(messages).content\n",
    "    return {\"tweet\":response, \"tweet_history\":[response]}\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1209545",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_tweet(state:TweetState):\n",
    "    messages=[\n",
    "        SystemMessage(content=\"You are a ruthless,no-laugh-given Twitter critic. you evaluate tweets based on humor, originality , virality and tweet format\"),\n",
    "        HumanMessage(content=f\"\"\" Evaluate the following tweet:\n",
    "                     Tweet: \"{state['tweet']}\" \n",
    "                     Rules:\n",
    "                        - The first draft should almost always fail (unless it’s truly viral-level).\n",
    "                        - Only approve if it is unique, laugh-out-loud funny, AND viral-worthy.\n",
    "                        - Be extra harsh on clichés, obvious jokes, or “safe” humor.\n",
    "                     Use the criteria below to evaluate the tweet:\n",
    "                     1. Originality - Is this fresh, or have you seen it a hundred times before? |\n",
    "                     2. Humor - Did it genuinely make you smile, laugh, or chuckle?\n",
    "                     3. Punchiness - Is it short, sharp, and scroll-stopping? |\n",
    "                     4. Virality Potential - Would people retweet or share it? |\n",
    "                     5. Format - Is it a well-formed tweet (not a setup-punchline joke, not a Q8A joke, and under 280 character.\n",
    "                     \n",
    "                     Auto-reject if:\n",
    "                     - Its written in question-answer format(e.g \"why did ...\" or \"what happen when...\"\n",
    "                        - It exceeds 20 characters.\n",
    "                        -it reads like traditional set-up punchline jokes\n",
    "                        -do not end with generic , throwaway or deflating lines that weaken the humor(e.g,\"Masterpieces\n",
    "\n",
    "                        ## respondonly only in structued format:\n",
    "                        -evaluation:\"approved\" or \"needs_improvement\"\n",
    "                        -feedback: one Paragraph explaining the strength and weakness\n",
    "                     \"\"\")\n",
    "    ]\n",
    "    response= structured_evaluator_llm.invoke(messages)\n",
    "    return {'evaluation':response.evaluation, 'feedback':response.feedback, \"feedback_history\":[response.feedback]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebc3eac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimized_tweet(state:TweetState):\n",
    "    messages=[\n",
    "        SystemMessage(content=\"You punch up tweets for virality and humor based on given feedback.\"),\n",
    "        HumanMessage(content=f\"\"\" Improve the following tweet based on this feedback:\n",
    "                     Feedback: \"{state['feedback']}\" \\n\n",
    "                     Topic: \"{state[\"topic\"]}\" \\n\n",
    "                     Tweet: \"{state['tweet']}\" ]n \n",
    "                     rewrite it as a short, viral-worthy tweet. Avoid Q&A style and under 200 charcter.\n",
    "                    \n",
    "                     \"\"\")\n",
    "    ]\n",
    "    response= optimizer_llm.invoke(messages).content\n",
    "    iteration=state[\"iteration\"]+1\n",
    "    return {'tweet':response, 'iteration':iteration}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71284a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_evaluation(state:TweetState):\n",
    "    if state['evaluation']==\"approved\" or state['iteration']==state['max_iteration']:\n",
    "        return 'approved'\n",
    "    else:\n",
    "        return 'needs_imporovement'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5da14867",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph=StateGraph(TweetState)\n",
    "graph.add_node('generate',generate_tweet)\n",
    "graph.add_node('evaluate',evaluate_tweet)\n",
    "graph.add_node('optimized',optimized_tweet)\n",
    "\n",
    "graph.add_edge(START,'generate')\n",
    "graph.add_edge('generate','evaluate')\n",
    "graph.add_conditional_edges('evaluate', route_evaluation, {'approved':END, 'needs_imporovement':'optimized'})\n",
    "graph.add_edge('optimized', 'evaluate')\n",
    "\n",
    "workflow=graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90925caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'evaluation': 'approved',\n",
      " 'feedback': 'This tweet is approved for its originality and humor. The '\n",
      "             'concept of AI writing self-deprecating tweets and experiencing '\n",
      "             'existential crises is fresh and engaging. The use of cat videos '\n",
      "             'as a comparison adds a relatable twist. The tweet is punchy, '\n",
      "             'engaging the audience with a humorous scenario in a concise '\n",
      "             'manner. It has the potential to be shared due to its relatable '\n",
      "             'and entertaining content. The format is well-structured and fits '\n",
      "             'within the character limit, making it suitable for Twitter.',\n",
      " 'feedback_history': ['This tweet is approved for its originality and humor. '\n",
      "                      'The concept of AI writing self-deprecating tweets and '\n",
      "                      'experiencing existential crises is fresh and engaging. '\n",
      "                      'The use of cat videos as a comparison adds a relatable '\n",
      "                      'twist. The tweet is punchy, engaging the audience with '\n",
      "                      'a humorous scenario in a concise manner. It has the '\n",
      "                      'potential to be shared due to its relatable and '\n",
      "                      'entertaining content. The format is well-structured and '\n",
      "                      'fits within the character limit, making it suitable for '\n",
      "                      'Twitter.'],\n",
      " 'max_iteration': 5,\n",
      " 'topic': 'AI writing tweets about itself airorkflow.invoke(initial_state) '\n",
      "          'will return a nested dict (your TweetState with lists of tweets and '\n",
      "          'feedbackport security lines First attempts will be too plain; '\n",
      "          'optimization needed for meme-like humor. ',\n",
      " 'tweet': '\"Just taught AI to write tweets about itself... now my timeline is '\n",
      "          \"full of self-deprecating jokes and existential crises. Should've \"\n",
      "          'stuck to cat videos. #AIstruggles\"',\n",
      " 'tweet_history': ['\"Just taught AI to write tweets about itself... now my '\n",
      "                   'timeline is full of self-deprecating jokes and existential '\n",
      "                   'crises. Should\\'ve stuck to cat videos. #AIstruggles\"']}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "initial_state={\n",
    "    \"topic\":\"AI writing tweets about itself airorkflow.invoke(initial_state) will return a nested dict (your TweetState with lists of tweets and feedbackport security lines First attempts will be too plain; optimization needed for meme-like humor. \",\n",
    "    \"iteration\":1,\n",
    "    \"max_iteration\":5\n",
    "} \n",
    "\n",
    "result= workflow.invoke(initial_state)\n",
    "\n",
    "pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f66ccaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Just taught AI to write tweets about itself... now my timeline is full of self-deprecating jokes and existential crises. Should've stuck to cat videos. #AIstruggles\"\n"
     ]
    }
   ],
   "source": [
    "for tweet in result['tweet_history']:\n",
    "    print(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181a0447",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff810090",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang-graph-learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
